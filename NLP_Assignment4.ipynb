{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T07:23:37.947152Z","iopub.status.busy":"2024-10-15T07:23:37.946698Z","iopub.status.idle":"2024-10-15T07:23:52.451184Z","shell.execute_reply":"2024-10-15T07:23:52.449870Z","shell.execute_reply.started":"2024-10-15T07:23:37.947110Z"},"trusted":true},"outputs":[],"source":["!pip install pandas nltk scikit-learn rouge -qq"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T07:23:52.454177Z","iopub.status.busy":"2024-10-15T07:23:52.453828Z","iopub.status.idle":"2024-10-15T07:23:58.252302Z","shell.execute_reply":"2024-10-15T07:23:58.250985Z","shell.execute_reply.started":"2024-10-15T07:23:52.454139Z"},"id":"_WuxAHxGY_6C","trusted":true},"outputs":[{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","from rouge import Rouge\n","import os\n","from collections import Counter\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt', quiet=True)\n","nltk.download('punkt_tab', quiet=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T07:23:58.254207Z","iopub.status.busy":"2024-10-15T07:23:58.253672Z","iopub.status.idle":"2024-10-15T07:23:58.270861Z","shell.execute_reply":"2024-10-15T07:23:58.269669Z","shell.execute_reply.started":"2024-10-15T07:23:58.254167Z"},"id":"mCIhYCOdY_6E","trusted":true},"outputs":[],"source":["# Define the BiLSTM model\n","class BiLSTMSummarizer(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n","        super(BiLSTMSummarizer, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.encoder = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n","        self.decoder = nn.LSTM(embedding_dim, hidden_dim * 2, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        batch_size = src.shape[0]\n","        trg_len = trg.shape[1]\n","        trg_vocab_size = self.fc.out_features\n","\n","        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(src.device)\n","\n","        embedded = self.embedding(src)\n","        enc_output, (hidden, cell) = self.encoder(embedded)\n","\n","        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1).unsqueeze(0)\n","        cell = torch.cat((cell[-2,:,:], cell[-1,:,:]), dim=1).unsqueeze(0)\n","\n","        input = trg[:, 0]\n","\n","        for t in range(1, trg_len):\n","            input_embedded = self.embedding(input).unsqueeze(1)\n","            output, (hidden, cell) = self.decoder(input_embedded, (hidden, cell))\n","            prediction = self.fc(output.squeeze(1))\n","            outputs[:, t] = prediction\n","\n","            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n","            top1 = prediction.argmax(1)\n","            input = trg[:, t] if teacher_force else top1\n","\n","        return outputs"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T07:23:58.274898Z","iopub.status.busy":"2024-10-15T07:23:58.274046Z","iopub.status.idle":"2024-10-15T07:23:58.287727Z","shell.execute_reply":"2024-10-15T07:23:58.286285Z","shell.execute_reply.started":"2024-10-15T07:23:58.274794Z"},"id":"gBEvTN3KY_6F","trusted":true},"outputs":[],"source":["# Custom dataset class\n","class SummarizationDataset(Dataset):\n","    def __init__(self, articles, summaries, vocab, max_length=100):\n","        self.articles = articles\n","        self.summaries = summaries\n","        self.vocab = vocab\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.articles)\n","\n","    def __getitem__(self, idx):\n","        article = self.articles[idx]\n","        summary = self.summaries[idx]\n","\n","        article_indices = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in article][:self.max_length-2] + [self.vocab['<eos>']]\n","        summary_indices = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in summary][:self.max_length-2] + [self.vocab['<eos>']]\n","\n","        article_indices = article_indices + [self.vocab['<pad>']] * (self.max_length - len(article_indices))\n","        summary_indices = summary_indices + [self.vocab['<pad>']] * (self.max_length - len(summary_indices))\n","\n","        return torch.tensor(article_indices), torch.tensor(summary_indices)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T07:23:58.289916Z","iopub.status.busy":"2024-10-15T07:23:58.289156Z","iopub.status.idle":"2024-10-15T07:23:58.301305Z","shell.execute_reply":"2024-10-15T07:23:58.300285Z","shell.execute_reply.started":"2024-10-15T07:23:58.289872Z"},"id":"tgn_dCLbY_6H","trusted":true},"outputs":[],"source":["file_path = r\"/kaggle/input/summarizationdataset/hindi_news_dataset.csv\"\n","\n","def load_data(file_path):\n","    df = pd.read_csv(file_path)\n","    return df['Headline'].tolist(), df['Content'].tolist()\n","\n","def tokenize(text):\n","    return word_tokenize(text.lower())\n","\n","def build_vocab(texts, min_freq=2):\n","    word_freq = Counter()\n","    for text in texts:\n","        word_freq.update(text)\n","\n","    vocab = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n","    \n","    for word, freq in word_freq.items():\n","        if freq >= min_freq:\n","            vocab[word] = len(vocab)\n","\n","    return vocab, {v: k for k, v in vocab.items()} "]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T07:23:58.303410Z","iopub.status.busy":"2024-10-15T07:23:58.302679Z","iopub.status.idle":"2024-10-15T07:26:36.902807Z","shell.execute_reply":"2024-10-15T07:26:36.901734Z","shell.execute_reply.started":"2024-10-15T07:23:58.303364Z"},"id":"lHfREZHwY_6I","trusted":true},"outputs":[],"source":["articles, summaries = load_data(file_path)\n","\n","tokenized_articles = [tokenize(article) for article in articles]\n","tokenized_summaries = [tokenize(summary) for summary in summaries]\n","\n","vocab, inv_vocab = build_vocab(tokenized_articles + tokenized_summaries)\n","\n","train_articles, test_articles, train_summaries, test_summaries = train_test_split(tokenized_articles, tokenized_summaries, test_size=0.2, random_state=42)\n","train_articles, val_articles, train_summaries, val_summaries = train_test_split(train_articles, train_summaries, test_size=0.1, random_state=42)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T07:26:36.904508Z","iopub.status.busy":"2024-10-15T07:26:36.904158Z","iopub.status.idle":"2024-10-15T07:26:36.929243Z","shell.execute_reply":"2024-10-15T07:26:36.928114Z","shell.execute_reply.started":"2024-10-15T07:26:36.904473Z"},"id":"CP18TCZfY_6J","trusted":true},"outputs":[],"source":["train_dataset = SummarizationDataset(train_articles, train_summaries, vocab, max_length=50)\n","val_dataset = SummarizationDataset(val_articles, val_summaries, vocab, max_length=50)\n","test_dataset = SummarizationDataset(test_articles, test_summaries, vocab, max_length=50)\n","\n","\n","# Create data loaders\n","train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=128)\n","test_loader = DataLoader(test_dataset, batch_size=128)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T07:26:36.931494Z","iopub.status.busy":"2024-10-15T07:26:36.930717Z","iopub.status.idle":"2024-10-15T07:26:38.244079Z","shell.execute_reply":"2024-10-15T07:26:38.243266Z","shell.execute_reply.started":"2024-10-15T07:26:36.931434Z"},"id":"-X7pLGHsY_6J","trusted":true},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","vocab_size = len(vocab)   \n","embedding_dim = 300      \n","hidden_dim = 512          \n","output_dim = vocab_size   \n","\n","model = BiLSTMSummarizer(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T07:26:38.245395Z","iopub.status.busy":"2024-10-15T07:26:38.245114Z","iopub.status.idle":"2024-10-15T07:26:38.255733Z","shell.execute_reply":"2024-10-15T07:26:38.254706Z","shell.execute_reply.started":"2024-10-15T07:26:38.245365Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using 2 GPUs!\n"]}],"source":["if torch.cuda.device_count() > 1:\n","    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n","    model = nn.DataParallel(model)\n","\n","model = model.to(device)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T07:26:38.259865Z","iopub.status.busy":"2024-10-15T07:26:38.259551Z","iopub.status.idle":"2024-10-15T07:26:38.268587Z","shell.execute_reply":"2024-10-15T07:26:38.267629Z","shell.execute_reply.started":"2024-10-15T07:26:38.259812Z"},"id":"UZGJGt5-Y_6K","trusted":true},"outputs":[],"source":["def train(model, iterator, optimizer, criterion, device, clip=1, teacher_forcing_ratio=0.5):\n","    model.train()\n","    epoch_loss = 0\n","    for batch in tqdm(iterator, desc=\"Training\"):\n","        src, trg = batch\n","        src, trg = src.to(device), trg.to(device)\n","\n","        optimizer.zero_grad()\n","        output = model(src, trg, teacher_forcing_ratio)\n","\n","        output_dim = output.shape[-1]\n","        output = output[:, 1:].reshape(-1, output_dim)\n","        trg = trg[:, 1:].reshape(-1)\n","\n","        loss = criterion(output, trg)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T07:26:38.270188Z","iopub.status.busy":"2024-10-15T07:26:38.269790Z","iopub.status.idle":"2024-10-15T07:26:38.280112Z","shell.execute_reply":"2024-10-15T07:26:38.279187Z","shell.execute_reply.started":"2024-10-15T07:26:38.270153Z"},"id":"sUtmmZLIY_6L","trusted":true},"outputs":[],"source":["def evaluate(model, iterator, criterion, device):\n","    model.eval()\n","    epoch_loss = 0\n","    with torch.no_grad():\n","        for batch in tqdm(iterator, desc=\"Evaluating\"):\n","            src, trg = batch\n","            src, trg = src.to(device), trg.to(device)\n","\n","            output = model(src, trg, 0)\n","\n","            output_dim = output.shape[-1]\n","            output = output[:, 1:].reshape(-1, output_dim)\n","            trg = trg[:, 1:].reshape(-1)\n","\n","            loss = criterion(output, trg)\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T07:26:38.282181Z","iopub.status.busy":"2024-10-15T07:26:38.281818Z","iopub.status.idle":"2024-10-15T07:26:38.327854Z","shell.execute_reply":"2024-10-15T07:26:38.327032Z","shell.execute_reply.started":"2024-10-15T07:26:38.282147Z"},"id":"dFZaxsetY_6L","trusted":true},"outputs":[],"source":["def beam_search(model, src, vocab, inv_vocab, beam_width=3, max_length=100, min_length=10, device='cpu'):\n","    model.eval()\n","    with torch.no_grad():\n","        embedded = model.embedding(src)  \n","        enc_output, (hidden, cell) = model.encoder(embedded) \n","\n","        # In case of bi-directional LSTM, combine the hidden states\n","        if model.encoder.bidirectional:\n","            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)  \n","            cell = torch.cat((cell[-2, :, :], cell[-1, :, :]), dim=1)        \n","        else:\n","            hidden = hidden[-1, :, :] \n","            cell = cell[-1, :, :]    \n","\n","        hidden = hidden.unsqueeze(0)  \n","        cell = cell.unsqueeze(0)     \n","\n","        beam = [([vocab['<sos>']], 0, hidden[:, 0:1, :], cell[:, 0:1, :])]\n","        complete_hypotheses = []\n","\n","        for t in range(max_length):\n","            new_beam = []\n","            for seq, score, hidden, cell in beam:\n","                if seq[-1] == vocab['<eos>'] and len(seq) >= min_length:\n","                    complete_hypotheses.append((seq, score))\n","                    continue\n","\n","                input = torch.LongTensor([seq[-1]]).unsqueeze(0).to(device)  \n","                input_embedded = model.embedding(input)\n","\n","                output, (hidden, cell) = model.decoder(input_embedded, (hidden, cell))\n","                predictions = model.fc(output.squeeze(1)) \n","\n","                if len(seq) < min_length:\n","                    predictions[0][vocab['<eos>']] = float('-inf')\n","\n","                top_preds = torch.topk(predictions, beam_width, dim=1)\n","\n","                for i in range(beam_width):\n","                    new_seq = seq + [top_preds.indices[0][i].item()]\n","                    new_score = score - top_preds.values[0][i].item() \n","                    new_hidden = hidden.clone()\n","                    new_cell = cell.clone()\n","                    new_beam.append((new_seq, new_score, new_hidden, new_cell))\n","\n","            beam = sorted(new_beam, key=lambda x: x[1])[:beam_width]\n","\n","            if len(complete_hypotheses) >= beam_width:\n","                break\n","\n","        complete_hypotheses = sorted(complete_hypotheses, key=lambda x: x[1])\n","        if complete_hypotheses:\n","            best_seq = complete_hypotheses[0][0]\n","        else:\n","            best_seq = beam[0][0]\n","\n","    return [inv_vocab[idx] for idx in best_seq if idx not in [vocab['<sos>'], vocab['<eos>'], vocab['<pad>']]]"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T07:26:38.329398Z","iopub.status.busy":"2024-10-15T07:26:38.329053Z","iopub.status.idle":"2024-10-15T07:26:38.341384Z","shell.execute_reply":"2024-10-15T07:26:38.340534Z","shell.execute_reply.started":"2024-10-15T07:26:38.329364Z"},"id":"iYMa8KYbY_6M","trusted":true},"outputs":[],"source":["def save_model(model, vocab, filepath):\n","    torch.save({\n","        'model_state_dict': model.state_dict(),\n","        'vocab': vocab\n","    }, filepath)\n","    print(f\"Model saved to {'/kaggle/working/'}\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T07:26:38.342637Z","iopub.status.busy":"2024-10-15T07:26:38.342362Z","iopub.status.idle":"2024-10-15T07:26:39.282400Z","shell.execute_reply":"2024-10-15T07:26:39.281332Z","shell.execute_reply.started":"2024-10-15T07:26:38.342607Z"},"id":"fCI_lkWhY_6M","trusted":true},"outputs":[],"source":["optimizer = optim.Adam(model.parameters())\n","criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T07:26:39.284219Z","iopub.status.busy":"2024-10-15T07:26:39.283726Z","iopub.status.idle":"2024-10-15T13:33:18.611322Z","shell.execute_reply":"2024-10-15T13:33:18.609237Z","shell.execute_reply.started":"2024-10-15T07:26:39.284183Z"},"id":"WyyssAHKY_6M","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/1044 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","Training: 100%|██████████| 1044/1044 [34:30<00:00,  1.98s/it]\n","Evaluating: 100%|██████████| 116/116 [01:56<00:00,  1.00s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 01\n","\tTrain Loss: 5.738\n","\t Val. Loss: 6.311\n","Model saved to /kaggle/working/\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1044/1044 [34:30<00:00,  1.98s/it]\n","Evaluating: 100%|██████████| 116/116 [01:55<00:00,  1.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 02\n","\tTrain Loss: 3.955\n","\t Val. Loss: 5.722\n","Model saved to /kaggle/working/\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1044/1044 [34:13<00:00,  1.97s/it]\n","Evaluating: 100%|██████████| 116/116 [01:54<00:00,  1.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 03\n","\tTrain Loss: 2.923\n","\t Val. Loss: 4.867\n","Model saved to /kaggle/working/\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1044/1044 [34:37<00:00,  1.99s/it]\n","Evaluating: 100%|██████████| 116/116 [01:57<00:00,  1.01s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 04\n","\tTrain Loss: 2.284\n","\t Val. Loss: 4.189\n","Model saved to /kaggle/working/\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1044/1044 [34:48<00:00,  2.00s/it]\n","Evaluating: 100%|██████████| 116/116 [01:58<00:00,  1.02s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 05\n","\tTrain Loss: 1.863\n","\t Val. Loss: 3.739\n","Model saved to /kaggle/working/\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1044/1044 [34:45<00:00,  2.00s/it]\n","Evaluating: 100%|██████████| 116/116 [01:57<00:00,  1.01s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 06\n","\tTrain Loss: 1.556\n","\t Val. Loss: 3.359\n","Model saved to /kaggle/working/\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1044/1044 [34:49<00:00,  2.00s/it]\n","Evaluating: 100%|██████████| 116/116 [01:58<00:00,  1.02s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 07\n","\tTrain Loss: 1.323\n","\t Val. Loss: 3.081\n","Model saved to /kaggle/working/\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1044/1044 [34:52<00:00,  2.00s/it]\n","Evaluating: 100%|██████████| 116/116 [01:58<00:00,  1.02s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 08\n","\tTrain Loss: 1.140\n","\t Val. Loss: 2.849\n","Model saved to /kaggle/working/\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1044/1044 [34:51<00:00,  2.00s/it]\n","Evaluating: 100%|██████████| 116/116 [01:58<00:00,  1.02s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 09\n","\tTrain Loss: 0.996\n","\t Val. Loss: 2.675\n","Model saved to /kaggle/working/\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1044/1044 [34:54<00:00,  2.01s/it]\n","Evaluating: 100%|██████████| 116/116 [01:59<00:00,  1.03s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 10\n","\tTrain Loss: 0.874\n","\t Val. Loss: 2.505\n","Model saved to /kaggle/working/\n"]}],"source":["num_epochs = 10\n","best_val_loss = float('inf')\n","for epoch in range(num_epochs):\n","    train_loss = train(model, train_loader, optimizer, criterion, device)\n","    val_loss = evaluate(model, val_loader, criterion, device)\n","    print(f'Epoch: {epoch+1:02}')\n","    print(f'\\tTrain Loss: {train_loss:.3f}')\n","    print(f'\\t Val. Loss: {val_loss:.3f}')\n","\n","    # Save model if validation loss improves\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        save_model(model, vocab, 'best_model.pth')"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T13:33:18.613910Z","iopub.status.busy":"2024-10-15T13:33:18.613415Z","iopub.status.idle":"2024-10-15T13:33:18.620203Z","shell.execute_reply":"2024-10-15T13:33:18.619256Z","shell.execute_reply.started":"2024-10-15T13:33:18.613853Z"},"id":"3Di3pobeY_6N","trusted":true},"outputs":[],"source":["def load_model(filepath, device):\n","    checkpoint = torch.load(filepath, map_location=device)\n","    vocab = checkpoint['vocab']\n","    model = BiLSTMSummarizer(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    return model, checkpoint"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T13:36:57.877563Z","iopub.status.busy":"2024-10-15T13:36:57.876746Z","iopub.status.idle":"2024-10-15T13:36:57.884513Z","shell.execute_reply":"2024-10-15T13:36:57.883511Z","shell.execute_reply.started":"2024-10-15T13:36:57.877519Z"},"trusted":true},"outputs":[],"source":["def load_model(path, device):\n","    checkpoint = torch.load(path, map_location=device)\n","    state_dict = checkpoint['model_state_dict']\n","\n","    new_state_dict = {}\n","    for k, v in state_dict.items():\n","        new_key = k.replace('module.', '') if 'module.' in k else k\n","        new_state_dict[new_key] = v\n","\n","    model = BiLSTMSummarizer(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n","    model.load_state_dict(new_state_dict)\n","    model.to(device)\n","\n","    optimizer_state = checkpoint.get('optimizer_state_dict', None)\n","\n","    return model, optimizer_state\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T13:37:00.341557Z","iopub.status.busy":"2024-10-15T13:37:00.341142Z","iopub.status.idle":"2024-10-15T13:43:16.879115Z","shell.execute_reply":"2024-10-15T13:43:16.878183Z","shell.execute_reply.started":"2024-10-15T13:37:00.341518Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_30/1206050121.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(path, map_location=device)\n","Evaluating: 100%|██████████| 290/290 [06:15<00:00,  1.29s/it]"]},{"name":"stdout","output_type":"stream","text":["Test Loss: 2.477\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["best_model, _ = load_model('best_model.pth', device)\n","best_model.eval() \n","\n","# Evaluate the model\n","test_loss = evaluate(best_model, test_loader, criterion, device)\n","print(f'Test Loss: {test_loss:.3f}')"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T13:43:16.881729Z","iopub.status.busy":"2024-10-15T13:43:16.881277Z","iopub.status.idle":"2024-10-15T13:44:33.768470Z","shell.execute_reply":"2024-10-15T13:44:33.767173Z","shell.execute_reply.started":"2024-10-15T13:43:16.881679Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating summaries: 100%|██████████| 290/290 [01:16<00:00,  3.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["ROUGE scores:\n","{'rouge-1': {'r': 0.797824581437502, 'p': 0.8224941994409078, 'f': 0.8085761290905409}, 'rouge-2': {'r': 0.7151748893102365, 'p': 0.7227557298798698, 'f': 0.7185470128082455}, 'rouge-l': {'r': 0.7767887418993216, 'p': 0.798882878044646, 'f': 0.7864074920678197}}\n"]}],"source":["rouge = Rouge()\n","best_model.eval() \n","\n","predictions = []\n","references = []\n","\n","with torch.no_grad():\n","    for batch in tqdm(test_loader, desc=\"Generating summaries\"):\n","        src, trg = batch\n","        src = src.to(device)\n","\n","        # Generate summary using beam search\n","        pred = beam_search(best_model, src, vocab, inv_vocab, min_length=10, device=device)\n","        predictions.append(' '.join(pred))\n","\n","        # Prepare reference text\n","        reference = ' '.join([inv_vocab[idx.item()] for idx in trg[0] if idx.item() not in [vocab['<sos>'], vocab['<eos>'], vocab['<pad>']]])\n","        references.append(reference)\n","\n","# Ensure all predictions meet the minimum length\n","min_length = 10  # Set your desired minimum length\n","predictions = [p if len(p.split()) >= min_length else p + ' ' + ' '.join(['<pad>'] * (min_length - len(p.split()))) for p in predictions]\n","\n","# Compute ROUGE scores\n","scores = rouge.get_scores(predictions, references, avg=True)\n","print(\"ROUGE scores:\")\n","print(scores)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T13:45:38.727710Z","iopub.status.busy":"2024-10-15T13:45:38.726671Z","iopub.status.idle":"2024-10-15T13:45:38.735673Z","shell.execute_reply":"2024-10-15T13:45:38.734620Z","shell.execute_reply.started":"2024-10-15T13:45:38.727661Z"},"id":"CMv-l-VmY_6O","trusted":true},"outputs":[],"source":["def summarize_text(model, vocab, inv_vocab, text, max_length=100, min_length=10, beam_width=3, device='cpu', debug=False):\n","    model.eval()\n","    tokens = tokenize(text)[:max_length]\n","    indices = [vocab['<sos>']] + [vocab.get(token, vocab['<unk>']) for token in tokens] + [vocab['<eos>']]\n","    src = torch.LongTensor(indices).unsqueeze(0).to(device)\n","\n","    summary = beam_search(model, src, vocab, inv_vocab, beam_width, max_length, min_length, device)\n","\n","    if debug:\n","        print(\"Input tokens:\", tokens)\n","        print(\"Input indices:\", indices)\n","        print(\"Generated indices:\", [vocab[word] for word in summary])\n","        print(\"Summary length:\", len(summary))\n","\n","    return ' '.join(summary)"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-10-15T13:55:32.058043Z","iopub.status.busy":"2024-10-15T13:55:32.057561Z","iopub.status.idle":"2024-10-15T13:55:32.330360Z","shell.execute_reply":"2024-10-15T13:55:32.329342Z","shell.execute_reply.started":"2024-10-15T13:55:32.057998Z"},"id":"_GrUGOTKY_6O","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Input tokens: ['भारतीय', 'अंतरिक्ष', 'अनुसंधान', 'संगठन', '(', 'isro', ')', 'ने', 'चंद्रयान-3', 'मिशन', 'को', 'सफलतापूर्वक', 'लॉन्च', 'किया', ',', 'जिसका', 'उद्देश्य', 'चंद्रमा', 'के', 'दक्षिणी', 'ध्रुव', 'पर', 'सुरक्षित', 'लैंडिंग', 'करना', 'है।', 'इस', 'मिशन', 'से', 'भारत', 'को', 'चंद्रमा', 'के', 'बारे', 'में', 'नई', 'जानकारी', 'प्राप्त', 'करने', 'और', 'अंतरिक्ष', 'अन्वेषण', 'में', 'अपनी', 'स्थिति', 'मजबूत', 'करने', 'की', 'उम्मीद', 'है।']\n","Input indices: [2, 392, 4036, 22731, 270, 6447, 17048, 6449, 83, 2231, 1410, 76, 4137, 2085, 32, 56, 1331, 9140, 2232, 12, 3266, 3267, 98, 512, 2235, 972, 28714, 1235, 1410, 37, 87, 76, 2232, 12, 4196, 10, 1480, 4197, 15901, 36, 73, 4036, 30152, 10, 405, 44, 13319, 36, 8, 2224, 28714, 3]\n","Generated indices: [2232, 8, 2234, 2235, 12, 30, 3180, 12, 355, 802, 2654, 83, 83, 211, 41, 852, 355, 5457, 6531, 26, 83, 87, 76, 4036, 983, 6447, 355, 10, 4606, 6449, 98, 1525, 56, 28715, 3180, 83, 2231, 8, 2235, 12, 4196, 10, 371, 12, 30, 634, 83, 747, 39, 65]\n","Summary length: 50\n","Generated Summary:\n","चंद्रमा की सफल लैंडिंग के बाद इसरो के पूर्व प्रमुख शरद ने ने कहा है कि पूर्व प्रधानमंत्री नरेंद्र मोदी ने भारत को अंतरिक्ष स्टेशन ( पूर्व में ट्विटर ) पर लिखा , `` इसरो ने चंद्रयान-3 की लैंडिंग के बारे में आने के बाद अमेरिका ने बधाई हो गए\n","Summary length: 50\n"]}],"source":["input_text = \"चंद्रमा पर चंद्रयान-3 की सफल लैंडिंग के बाद, इसरो के पूर्व प्रमुख शरद ने खुशी व्यक्त की। उन्होंने कहा कि इस ऐतिहासिक उपलब्धि के लिए पूर्व प्रधानमंत्री नरेंद्र मोदी ने सोशल मीडिया पर अपनी बधाई दी। उन्होंने कहा कि अमेरिका ने भी इसरो को चंद्रयान-3 की सफल लैंडिंग पर बधाई दी है, जो भारत की अंतरिक्ष प्रगति का महत्वपूर्ण संकेत है।\"\n","summary = summarize_text(trained_model, vocab, inv_vocab, input_text, min_length=10, device=device, debug=True)\n","print(\"Generated Summary:\")\n","print(summary)\n","print(\"Summary length:\", len(summary.split()))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5837581,"sourceId":9575760,"sourceType":"datasetVersion"},{"datasetId":5853162,"sourceId":9595495,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
